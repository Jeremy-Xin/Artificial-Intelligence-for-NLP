{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient(ip='192.168.0.105')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = bc.encode(['你好'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans2 = bc.encode(['\"哎，想当年来佘山的时候，啥都没有，三品香算镇上最大看起来最像样的饭店了。菜品多，有点太多，感觉啥都有，杂都不足以形容。随便点些，居然口味什么的都好还可以，价钱自然是便宜当震惊。元宝虾和椒盐九肚鱼都不错吃。不过近来几次么，味道明显没以前好了。冷餐里面一个凉拌海带丝还可以，酸酸甜甜的。镇上也有了些别的大点的饭店，所以不是每次必来了。对了，这家的生意一如既往的超级好，不定位基本吃不到。不过佘山这边的人吃晚饭很早的，所以稍微晚点去就很空了。\"'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basemodel.ipynb\n",
      "bert_client.ipynb\n",
      "comment-classification\n",
      "comment-classification.zip\n",
      "gru_cnn_model.ipynb\n",
      "input\n",
      "models\n",
      "preprocess.ipynb\n",
      "processed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \".\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, GRU, Conv1D, MaxPool1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_csv('./input/sentiment_analysis_trainingset.csv')\n",
    "train_y_processed = pd.read_csv('./processed/train_y_processed')\n",
    "val_x = pd.read_csv('./input/sentiment_analysis_validationset.csv')\n",
    "val_y_processed = pd.read_csv('./processed/val_y_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = train_x.content.tolist()\n",
    "val_list = val_x.content.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xincj/.virtualenvs/python3env/lib/python3.6/site-packages/bert_serving/client/__init__.py:285: UserWarning: some of your sentences have more tokens than \"max_seq_len=25\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    }
   ],
   "source": [
    "train_bert_sentence_embedding1 = bc.encode(train_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"file_name.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.902295</td>\n",
       "      <td>0.050484</td>\n",
       "      <td>-0.172733</td>\n",
       "      <td>0.163340</td>\n",
       "      <td>0.216406</td>\n",
       "      <td>-0.884055</td>\n",
       "      <td>-0.113720</td>\n",
       "      <td>0.193803</td>\n",
       "      <td>-0.024965</td>\n",
       "      <td>0.047187</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007610</td>\n",
       "      <td>0.245897</td>\n",
       "      <td>-0.025347</td>\n",
       "      <td>-0.055929</td>\n",
       "      <td>-0.028406</td>\n",
       "      <td>0.041145</td>\n",
       "      <td>0.058657</td>\n",
       "      <td>0.177345</td>\n",
       "      <td>0.279107</td>\n",
       "      <td>-0.214845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.870421</td>\n",
       "      <td>0.025380</td>\n",
       "      <td>-0.282298</td>\n",
       "      <td>0.084426</td>\n",
       "      <td>0.207569</td>\n",
       "      <td>-0.944237</td>\n",
       "      <td>-0.350477</td>\n",
       "      <td>0.266563</td>\n",
       "      <td>-0.001834</td>\n",
       "      <td>-0.254577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124792</td>\n",
       "      <td>0.022239</td>\n",
       "      <td>-0.065761</td>\n",
       "      <td>0.246078</td>\n",
       "      <td>-0.279375</td>\n",
       "      <td>-0.025940</td>\n",
       "      <td>0.330564</td>\n",
       "      <td>0.122222</td>\n",
       "      <td>0.287852</td>\n",
       "      <td>-0.305351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.191080</td>\n",
       "      <td>0.005152</td>\n",
       "      <td>-0.711179</td>\n",
       "      <td>-0.109769</td>\n",
       "      <td>0.249980</td>\n",
       "      <td>-1.400326</td>\n",
       "      <td>-0.272405</td>\n",
       "      <td>0.510504</td>\n",
       "      <td>-0.163640</td>\n",
       "      <td>-0.549664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312192</td>\n",
       "      <td>-0.321882</td>\n",
       "      <td>-0.142657</td>\n",
       "      <td>0.606598</td>\n",
       "      <td>-0.177795</td>\n",
       "      <td>0.178407</td>\n",
       "      <td>0.043636</td>\n",
       "      <td>0.180464</td>\n",
       "      <td>0.022202</td>\n",
       "      <td>-0.222446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.843061</td>\n",
       "      <td>0.215399</td>\n",
       "      <td>-0.363675</td>\n",
       "      <td>0.097502</td>\n",
       "      <td>0.104311</td>\n",
       "      <td>-1.285999</td>\n",
       "      <td>-0.267110</td>\n",
       "      <td>0.339181</td>\n",
       "      <td>-0.332437</td>\n",
       "      <td>-0.306840</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.291819</td>\n",
       "      <td>-0.133949</td>\n",
       "      <td>0.081637</td>\n",
       "      <td>0.067365</td>\n",
       "      <td>-0.226054</td>\n",
       "      <td>0.288608</td>\n",
       "      <td>0.105808</td>\n",
       "      <td>0.201949</td>\n",
       "      <td>0.300128</td>\n",
       "      <td>-0.199779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.994475</td>\n",
       "      <td>0.233835</td>\n",
       "      <td>-0.323046</td>\n",
       "      <td>0.141066</td>\n",
       "      <td>-0.093911</td>\n",
       "      <td>-1.003276</td>\n",
       "      <td>-0.222923</td>\n",
       "      <td>0.478165</td>\n",
       "      <td>0.023107</td>\n",
       "      <td>-0.218585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003418</td>\n",
       "      <td>-0.036019</td>\n",
       "      <td>-0.388153</td>\n",
       "      <td>0.104437</td>\n",
       "      <td>-0.101356</td>\n",
       "      <td>0.112896</td>\n",
       "      <td>0.107625</td>\n",
       "      <td>0.181771</td>\n",
       "      <td>0.284554</td>\n",
       "      <td>0.082852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.085047</td>\n",
       "      <td>-0.049393</td>\n",
       "      <td>-0.447100</td>\n",
       "      <td>-0.063946</td>\n",
       "      <td>0.213721</td>\n",
       "      <td>-1.007541</td>\n",
       "      <td>-0.149813</td>\n",
       "      <td>0.515427</td>\n",
       "      <td>-0.276237</td>\n",
       "      <td>-0.345025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040773</td>\n",
       "      <td>-0.094097</td>\n",
       "      <td>-0.360262</td>\n",
       "      <td>0.170443</td>\n",
       "      <td>-0.122729</td>\n",
       "      <td>0.188749</td>\n",
       "      <td>-0.014536</td>\n",
       "      <td>0.186082</td>\n",
       "      <td>0.372076</td>\n",
       "      <td>-0.184510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.778790</td>\n",
       "      <td>0.072899</td>\n",
       "      <td>-0.060067</td>\n",
       "      <td>0.271493</td>\n",
       "      <td>0.097915</td>\n",
       "      <td>-0.995104</td>\n",
       "      <td>-0.063503</td>\n",
       "      <td>0.376292</td>\n",
       "      <td>-0.293974</td>\n",
       "      <td>-0.393685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108379</td>\n",
       "      <td>0.057622</td>\n",
       "      <td>-0.166568</td>\n",
       "      <td>0.269459</td>\n",
       "      <td>-0.135326</td>\n",
       "      <td>0.206665</td>\n",
       "      <td>0.169650</td>\n",
       "      <td>0.282393</td>\n",
       "      <td>0.359678</td>\n",
       "      <td>-0.222400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.702339</td>\n",
       "      <td>0.137238</td>\n",
       "      <td>-0.330530</td>\n",
       "      <td>-0.123078</td>\n",
       "      <td>0.157724</td>\n",
       "      <td>-0.991845</td>\n",
       "      <td>-0.122050</td>\n",
       "      <td>0.106353</td>\n",
       "      <td>-0.138912</td>\n",
       "      <td>-0.366143</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.209375</td>\n",
       "      <td>0.172143</td>\n",
       "      <td>-0.152252</td>\n",
       "      <td>0.232445</td>\n",
       "      <td>-0.103964</td>\n",
       "      <td>0.230836</td>\n",
       "      <td>0.130483</td>\n",
       "      <td>0.080095</td>\n",
       "      <td>0.105732</td>\n",
       "      <td>-0.224641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.942434</td>\n",
       "      <td>0.028669</td>\n",
       "      <td>-0.158299</td>\n",
       "      <td>0.172286</td>\n",
       "      <td>0.100960</td>\n",
       "      <td>-0.724702</td>\n",
       "      <td>-0.166792</td>\n",
       "      <td>0.044498</td>\n",
       "      <td>0.031871</td>\n",
       "      <td>-0.142305</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028930</td>\n",
       "      <td>0.142949</td>\n",
       "      <td>0.047249</td>\n",
       "      <td>0.405469</td>\n",
       "      <td>-0.310105</td>\n",
       "      <td>-0.002172</td>\n",
       "      <td>0.031438</td>\n",
       "      <td>0.079474</td>\n",
       "      <td>0.242368</td>\n",
       "      <td>-0.103473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.803228</td>\n",
       "      <td>-0.192137</td>\n",
       "      <td>0.031246</td>\n",
       "      <td>-0.078509</td>\n",
       "      <td>0.312756</td>\n",
       "      <td>-0.756581</td>\n",
       "      <td>-0.256271</td>\n",
       "      <td>0.288060</td>\n",
       "      <td>0.019070</td>\n",
       "      <td>-0.298895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226157</td>\n",
       "      <td>0.228943</td>\n",
       "      <td>-0.068472</td>\n",
       "      <td>0.222720</td>\n",
       "      <td>-0.396131</td>\n",
       "      <td>0.099331</td>\n",
       "      <td>0.103129</td>\n",
       "      <td>0.118500</td>\n",
       "      <td>0.195484</td>\n",
       "      <td>-0.088543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.902295  0.050484 -0.172733  0.163340  0.216406 -0.884055 -0.113720   \n",
       "1  0.870421  0.025380 -0.282298  0.084426  0.207569 -0.944237 -0.350477   \n",
       "2  1.191080  0.005152 -0.711179 -0.109769  0.249980 -1.400326 -0.272405   \n",
       "3  0.843061  0.215399 -0.363675  0.097502  0.104311 -1.285999 -0.267110   \n",
       "4  0.994475  0.233835 -0.323046  0.141066 -0.093911 -1.003276 -0.222923   \n",
       "5  1.085047 -0.049393 -0.447100 -0.063946  0.213721 -1.007541 -0.149813   \n",
       "6  0.778790  0.072899 -0.060067  0.271493  0.097915 -0.995104 -0.063503   \n",
       "7  0.702339  0.137238 -0.330530 -0.123078  0.157724 -0.991845 -0.122050   \n",
       "8  0.942434  0.028669 -0.158299  0.172286  0.100960 -0.724702 -0.166792   \n",
       "9  0.803228 -0.192137  0.031246 -0.078509  0.312756 -0.756581 -0.256271   \n",
       "\n",
       "        7         8         9      ...          758       759       760  \\\n",
       "0  0.193803 -0.024965  0.047187    ...    -0.007610  0.245897 -0.025347   \n",
       "1  0.266563 -0.001834 -0.254577    ...    -0.124792  0.022239 -0.065761   \n",
       "2  0.510504 -0.163640 -0.549664    ...    -0.312192 -0.321882 -0.142657   \n",
       "3  0.339181 -0.332437 -0.306840    ...    -0.291819 -0.133949  0.081637   \n",
       "4  0.478165  0.023107 -0.218585    ...    -0.003418 -0.036019 -0.388153   \n",
       "5  0.515427 -0.276237 -0.345025    ...    -0.040773 -0.094097 -0.360262   \n",
       "6  0.376292 -0.293974 -0.393685    ...     0.108379  0.057622 -0.166568   \n",
       "7  0.106353 -0.138912 -0.366143    ...    -0.209375  0.172143 -0.152252   \n",
       "8  0.044498  0.031871 -0.142305    ...    -0.028930  0.142949  0.047249   \n",
       "9  0.288060  0.019070 -0.298895    ...     0.226157  0.228943 -0.068472   \n",
       "\n",
       "        761       762       763       764       765       766       767  \n",
       "0 -0.055929 -0.028406  0.041145  0.058657  0.177345  0.279107 -0.214845  \n",
       "1  0.246078 -0.279375 -0.025940  0.330564  0.122222  0.287852 -0.305351  \n",
       "2  0.606598 -0.177795  0.178407  0.043636  0.180464  0.022202 -0.222446  \n",
       "3  0.067365 -0.226054  0.288608  0.105808  0.201949  0.300128 -0.199779  \n",
       "4  0.104437 -0.101356  0.112896  0.107625  0.181771  0.284554  0.082852  \n",
       "5  0.170443 -0.122729  0.188749 -0.014536  0.186082  0.372076 -0.184510  \n",
       "6  0.269459 -0.135326  0.206665  0.169650  0.282393  0.359678 -0.222400  \n",
       "7  0.232445 -0.103964  0.230836  0.130483  0.080095  0.105732 -0.224641  \n",
       "8  0.405469 -0.310105 -0.002172  0.031438  0.079474  0.242368 -0.103473  \n",
       "9  0.222720 -0.396131  0.099331  0.103129  0.118500  0.195484 -0.088543  \n",
       "\n",
       "[10 rows x 768 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xincj/.virtualenvs/python3env/lib/python3.6/site-packages/bert_serving/client/__init__.py:285: UserWarning: some of your sentences have more tokens than \"max_seq_len=25\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-e978bace8a3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1050\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_bert_sentence_embedding1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./bert_embedding/{}.csv\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_bert_sentence_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/python3env/lib/python3.6/site-packages/bert_serving/client/__init__.py\u001b[0m in \u001b[0;36marg_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceiver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRCVTIMEO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_e\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 t_e = TimeoutError(\n",
      "\u001b[0;32m~/.virtualenvs/python3env/lib/python3.6/site-packages/bert_serving/client/__init__.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, texts, blocking, is_tokenized, show_tokens)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_info_available\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshow_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/python3env/lib/python3.6/site-packages/bert_serving/client/__init__.py\u001b[0m in \u001b[0;36m_recv_ndarray\u001b[0;34m(self, wait_for_req_id)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_for_req_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_for_req_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0marr_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsonapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dtype'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/python3env/lib/python3.6/site-packages/bert_serving/client/__init__.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, wait_for_req_id)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;31m# receive a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceiver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/python3env/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0many\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mSocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mmight\u001b[0m \u001b[0mfail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \"\"\"\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRCVMORE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/python3env/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(0,1050):\n",
    "    train_bert_sentence_embedding1 = batch = bc.encode(train_list[i*100: i*100 + 100])\n",
    "    np.savetxt(\"./bert_embedding/{}.csv\".format(i), train_bert_sentence_embedding, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(768,)))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(80, activation=\"sigmoid\"))\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 80)                41040     \n",
      "=================================================================\n",
      "Total params: 697,424\n",
      "Trainable params: 697,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "for i in range(0, 250):\n",
    "    data = pd.read_csv(\"./bert_embedding/{}.csv\".format(i), header=None)\n",
    "    train_x.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_embeddings_train = pd.concat(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = train_y_processed.iloc[0:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x = []\n",
    "for i in range(250, 290):\n",
    "    data = pd.read_csv(\"./bert_embedding/{}.csv\".format(i), header=None)\n",
    "    val_x.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings_val = pd.concat(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_val = train_y_processed.iloc[25000:29000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 80)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 4000 samples\n",
      "Epoch 1/16\n",
      "25000/25000 [==============================] - 9s 352us/step - loss: 0.3634 - acc: 0.8422 - f1: 0.6658 - val_loss: 0.3615 - val_acc: 0.8426 - val_f1: 0.6601\n",
      "Epoch 2/16\n",
      "25000/25000 [==============================] - 8s 339us/step - loss: 0.3607 - acc: 0.8434 - f1: 0.6698 - val_loss: 0.3621 - val_acc: 0.8420 - val_f1: 0.6534\n",
      "Epoch 3/16\n",
      "25000/25000 [==============================] - 9s 341us/step - loss: 0.3604 - acc: 0.8433 - f1: 0.6693 - val_loss: 0.3614 - val_acc: 0.8429 - val_f1: 0.6778\n",
      "Epoch 4/16\n",
      "25000/25000 [==============================] - 9s 340us/step - loss: 0.3600 - acc: 0.8434 - f1: 0.6706 - val_loss: 0.3612 - val_acc: 0.8427 - val_f1: 0.6692\n",
      "Epoch 5/16\n",
      "25000/25000 [==============================] - 9s 342us/step - loss: 0.3599 - acc: 0.8434 - f1: 0.6719 - val_loss: 0.3618 - val_acc: 0.8424 - val_f1: 0.6656\n",
      "Epoch 6/16\n",
      "25000/25000 [==============================] - 9s 341us/step - loss: 0.3598 - acc: 0.8434 - f1: 0.6723 - val_loss: 0.3610 - val_acc: 0.8430 - val_f1: 0.6717\n",
      "Epoch 7/16\n",
      "25000/25000 [==============================] - 8s 335us/step - loss: 0.3598 - acc: 0.8435 - f1: 0.6720 - val_loss: 0.3612 - val_acc: 0.8424 - val_f1: 0.6626\n",
      "Epoch 8/16\n",
      "25000/25000 [==============================] - 9s 346us/step - loss: 0.3597 - acc: 0.8435 - f1: 0.6724 - val_loss: 0.3610 - val_acc: 0.8427 - val_f1: 0.6675\n",
      "Epoch 9/16\n",
      "25000/25000 [==============================] - 8s 336us/step - loss: 0.3597 - acc: 0.8435 - f1: 0.6728 - val_loss: 0.3609 - val_acc: 0.8428 - val_f1: 0.6691\n",
      "Epoch 10/16\n",
      "25000/25000 [==============================] - 7s 297us/step - loss: 0.3597 - acc: 0.8435 - f1: 0.6716 - val_loss: 0.3610 - val_acc: 0.8429 - val_f1: 0.6774\n",
      "Epoch 11/16\n",
      "25000/25000 [==============================] - 9s 344us/step - loss: 0.3596 - acc: 0.8436 - f1: 0.6741 - val_loss: 0.3612 - val_acc: 0.8428 - val_f1: 0.6682\n",
      "Epoch 12/16\n",
      "25000/25000 [==============================] - 9s 347us/step - loss: 0.3596 - acc: 0.8435 - f1: 0.6735 - val_loss: 0.3608 - val_acc: 0.8429 - val_f1: 0.6772\n",
      "Epoch 13/16\n",
      "25000/25000 [==============================] - 9s 350us/step - loss: 0.3596 - acc: 0.8435 - f1: 0.6725 - val_loss: 0.3608 - val_acc: 0.8427 - val_f1: 0.6619\n",
      "Epoch 14/16\n",
      "25000/25000 [==============================] - 9s 350us/step - loss: 0.3596 - acc: 0.8435 - f1: 0.6725 - val_loss: 0.3607 - val_acc: 0.8428 - val_f1: 0.6691\n",
      "Epoch 15/16\n",
      "25000/25000 [==============================] - 9s 349us/step - loss: 0.3596 - acc: 0.8435 - f1: 0.6737 - val_loss: 0.3609 - val_acc: 0.8430 - val_f1: 0.6771\n",
      "Epoch 16/16\n",
      "25000/25000 [==============================] - 9s 347us/step - loss: 0.3595 - acc: 0.8435 - f1: 0.6742 - val_loss: 0.3611 - val_acc: 0.8426 - val_f1: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12d76ebe0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 16\n",
    "\n",
    "model.fit(bert_embeddings_train, label_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=[bert_embeddings_val, label_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
