{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basemodel.ipynb\n",
      "comment-classification\n",
      "comment-classification.zip\n",
      "gru_cnn_model.ipynb\n",
      "input\n",
      "models\n",
      "preprocess.ipynb\n",
      "processed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \".\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, GRU, Conv1D, MaxPool1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_processed = pd.read_csv('./processed/train_x_processed')\n",
    "train_y_processed = pd.read_csv('./processed/train_y_processed')\n",
    "val_x_processed = pd.read_csv('./processed/val_x_processed')\n",
    "val_y_processed = pd.read_csv('./processed/val_y_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 200\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_x_processed.content))\n",
    "\n",
    "train_x_token = tokenizer.texts_to_sequences(train_x_processed.content)\n",
    "val_x_token = tokenizer.texts_to_sequences(val_x_processed.content)\n",
    "\n",
    "train_X = sequence.pad_sequences(train_x_token, maxlen=maxlen)\n",
    "val_X = sequence.pad_sequences(val_x_token, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(GRU(128, return_sequences=True, recurrent_dropout=0.1)))\n",
    "model.add(Bidirectional(GRU(128, return_sequences=True, recurrent_dropout=0.1)))\n",
    "model.add(Conv1D(64, kernel_size=3, kernel_initializer='glorot_uniform', padding='valid'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(80, activation=\"sigmoid\"))\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 200, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 200, 256)          197376    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 198, 64)           49216     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                3250      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 80)                4080      \n",
      "=================================================================\n",
      "Total params: 1,533,922\n",
      "Trainable params: 1,533,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105000 samples, validate on 15000 samples\n",
      "Epoch 1/2\n",
      "105000/105000 [==============================] - 951s 9ms/step - loss: 0.2454 - acc: 0.9009 - f1: 0.7916 - val_loss: 0.2267 - val_acc: 0.9089 - val_f1: 0.8085\n",
      "Epoch 2/2\n",
      "105000/105000 [==============================] - 968s 9ms/step - loss: 0.2369 - acc: 0.9048 - f1: 0.7998 - val_loss: 0.2214 - val_acc: 0.9123 - val_f1: 0.8156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x142265eb8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "model.fit(train_X, train_y_processed,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=[val_X, val_y_processed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./models/gru_cnn_6epochs.h5', custom_objects={'f1':f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105000 samples, validate on 15000 samples\n",
      "Epoch 1/2\n",
      "105000/105000 [==============================] - 953s 9ms/step - loss: 0.2116 - acc: 0.9140 - f1: 0.8192 - val_loss: 0.2283 - val_acc: 0.9119 - val_f1: 0.8146\n",
      "Epoch 2/2\n",
      "105000/105000 [==============================] - 959s 9ms/step - loss: 0.2073 - acc: 0.9158 - f1: 0.8230 - val_loss: 0.2306 - val_acc: 0.9109 - val_f1: 0.8127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14c064e48>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "model.fit(train_X, train_y_processed,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=[val_X, val_y_processed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/gru_cnn_8epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_prediction = model.predict(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.9687237e-01, 7.8557758e-04, 4.8008442e-04, 2.4861668e-03,\n",
       "       9.9025118e-01, 1.2394754e-04, 4.1677093e-04, 9.1625769e-03,\n",
       "       9.9789268e-01, 9.9889957e-04, 9.6633163e-04, 1.4306314e-03,\n",
       "       8.8452309e-01, 2.6132138e-02, 3.6940087e-02, 3.4728110e-02,\n",
       "       9.9391705e-01, 1.2250890e-03, 4.4313348e-03, 3.2886972e-03,\n",
       "       9.9962080e-01, 1.1678530e-04, 2.7501522e-04, 2.6853738e-04,\n",
       "       9.8723340e-01, 4.9626878e-03, 6.5971520e-03, 5.7990672e-03,\n",
       "       1.1054547e-01, 5.5491991e-02, 2.8927964e-01, 6.1916935e-01,\n",
       "       8.5887027e-01, 1.3877025e-02, 2.1185970e-02, 8.4489748e-02,\n",
       "       9.9730790e-01, 6.4987963e-04, 3.5331638e-03, 1.7852376e-03,\n",
       "       9.9775946e-01, 2.8790708e-03, 3.0096381e-03, 9.0758991e-04,\n",
       "       9.9729532e-01, 2.1226762e-03, 3.2921514e-04, 7.2395697e-04,\n",
       "       9.9449509e-01, 3.7263811e-03, 1.6867676e-03, 2.1181754e-03,\n",
       "       9.9088895e-01, 1.1975194e-02, 1.9541115e-03, 1.9001622e-03,\n",
       "       4.8325518e-01, 4.6592545e-02, 6.5133207e-02, 3.3793047e-01,\n",
       "       1.2087071e-02, 4.0807940e-02, 7.9346359e-01, 1.6079482e-01,\n",
       "       8.5578901e-01, 5.4139171e-02, 6.1536655e-02, 5.7630677e-02,\n",
       "       9.5728105e-01, 1.8937636e-02, 1.6460331e-02, 1.4687339e-02,\n",
       "       2.0229209e-02, 2.9438850e-02, 6.0062784e-01, 3.2745683e-01,\n",
       "       9.7292793e-01, 1.3765523e-03, 1.1325809e-02, 1.3495095e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_prediction[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(row_index, label_index):\n",
    "    return val_prediction[row_index][label_index * 4 - 4: label_index * 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00528253, 0.01328158, 0.01953338, 0.98854136], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(23, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(GRU(128, return_sequences=True, recurrent_dropout=0.1)))\n",
    "model.add(Conv1D(64, kernel_size=3, kernel_initializer='glorot_uniform', padding='valid'))\n",
    "model.add(MaxPool1D(pool_size=3))\n",
    "model.add(Conv1D(64, kernel_size=3, kernel_initializer='glorot_uniform', padding='valid'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(80, activation=\"sigmoid\"))\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 200, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 200, 256)          197376    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 198, 64)           49216     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 66, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 64, 64)            12352     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 80)                5200      \n",
      "=================================================================\n",
      "Total params: 1,552,976\n",
      "Trainable params: 1,552,720\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105000 samples, validate on 15000 samples\n",
      "Epoch 1/4\n",
      "105000/105000 [==============================] - 942s 9ms/step - loss: 0.3496 - acc: 0.8463 - f1: 0.6860 - val_loss: 0.2989 - val_acc: 0.8726 - val_f1: 0.7315\n",
      "Epoch 2/4\n",
      "105000/105000 [==============================] - 921s 9ms/step - loss: 0.2933 - acc: 0.8768 - f1: 0.7381 - val_loss: 0.2703 - val_acc: 0.8881 - val_f1: 0.7629\n",
      "Epoch 3/4\n",
      "105000/105000 [==============================] - 955s 9ms/step - loss: 0.2745 - acc: 0.8863 - f1: 0.7588 - val_loss: 0.2558 - val_acc: 0.8943 - val_f1: 0.7764\n",
      "Epoch 4/4\n",
      "105000/105000 [==============================] - 1017s 10ms/step - loss: 0.2647 - acc: 0.8907 - f1: 0.7684 - val_loss: 0.2494 - val_acc: 0.8985 - val_f1: 0.7835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1479e25c0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 4\n",
    "\n",
    "model.fit(train_X, train_y_processed,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=[val_X, val_y_processed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/gru_cnn_deeper_4epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105000 samples, validate on 15000 samples\n",
      "Epoch 1/2\n",
      "105000/105000 [==============================] - 944s 9ms/step - loss: 0.2417 - acc: 0.9026 - f1: 0.7947 - val_loss: 0.2342 - val_acc: 0.9062 - val_f1: 0.8021\n",
      "Epoch 2/2\n",
      "105000/105000 [==============================] - 950s 9ms/step - loss: 0.2360 - acc: 0.9054 - f1: 0.8008 - val_loss: 0.2328 - val_acc: 0.9069 - val_f1: 0.8037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x149fcc4a8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 4\n",
    "\n",
    "model.fit(train_X, train_y_processed,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=[val_X, val_y_processed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/gru_cnn_deeper_8epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105000 samples, validate on 15000 samples\n",
      "Epoch 1/4\n",
      "105000/105000 [==============================] - 957s 9ms/step - loss: 0.2307 - acc: 0.9078 - f1: 0.8062 - val_loss: 0.2328 - val_acc: 0.9074 - val_f1: 0.8051\n",
      "Epoch 2/4\n",
      "105000/105000 [==============================] - 967s 9ms/step - loss: 0.2258 - acc: 0.9099 - f1: 0.8107 - val_loss: 0.2336 - val_acc: 0.9078 - val_f1: 0.8063\n",
      "Epoch 3/4\n",
      "105000/105000 [==============================] - 949s 9ms/step - loss: 0.2215 - acc: 0.9119 - f1: 0.8150 - val_loss: 0.2346 - val_acc: 0.9075 - val_f1: 0.8059\n",
      "Epoch 4/4\n",
      "105000/105000 [==============================] - 967s 9ms/step - loss: 0.2174 - acc: 0.9136 - f1: 0.8188 - val_loss: 0.2373 - val_acc: 0.9075 - val_f1: 0.8054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x149fccf28>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 4\n",
    "\n",
    "model.fit(train_X, train_y_processed,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=[val_X, val_y_processed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/gru_cnn_deeper_12epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
